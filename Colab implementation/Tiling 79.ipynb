{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCerkQNcn9T8"
      },
      "source": [
        "## Cuda C Tile Size 79"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZwyzMwRoAlG",
        "outputId": "83635567-8878-4856-a12b-ccc8715109b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ptxas error   : Entry function '_Z15MatrixMultTiledPfS_S_iii' uses too much shared data (0xc308 bytes, 0xc000 max)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#define TITLE_SIZE 79\n",
        "\n",
        "// Kernel function with tiling\n",
        "__global__ void MatrixMultTiled(float* M, float* N, float* P, int width, int height, int depth)\n",
        "{\n",
        "    __shared__ float Ms[TITLE_SIZE][TITLE_SIZE];\n",
        "    __shared__ float Ns[TITLE_SIZE][TITLE_SIZE];\n",
        "\n",
        "    int bx = blockIdx.x; \n",
        "    int by = blockIdx.y;\n",
        "    int tx = threadIdx.x; \n",
        "    int ty = threadIdx.y;\n",
        "\n",
        "    // Identify the row and column of the P element to work on\n",
        "    int Row = by * TITLE_SIZE + ty;\n",
        "    int Col = bx * TITLE_SIZE + tx;\n",
        "\n",
        "    float Pvalue = 0;\n",
        "\n",
        "    // Loop over the M and N tiles required to compute the P element\n",
        "    for (int m = 0; m < (width-1)/TITLE_SIZE+1; ++m) {\n",
        "\n",
        "        // Collaborative loading of M and N tiles into shared memory\n",
        "        if (Row < width && m*TITLE_SIZE+tx < height) \n",
        "            Ms[ty][tx] = M[Row*width + (m*TITLE_SIZE + tx)];\n",
        "        else\n",
        "            Ms[ty][tx] = 0.0;\n",
        "\n",
        "        if (m*TITLE_SIZE+ty < width && Col < depth)\n",
        "            Ns[ty][tx] = N[(m*TITLE_SIZE + ty)*depth + Col];\n",
        "        else\n",
        "            Ns[ty][tx] = 0.0;\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int k = 0; k < TITLE_SIZE; ++k) {\n",
        "            Pvalue += Ms[ty][k] * Ns[k][tx];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (Row < width && Col < depth)\n",
        "        P[Row*depth + Col] = Pvalue;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int M = 1500;\n",
        "    int N = 1500;\n",
        "    int K = 1500;\n",
        "\n",
        "    //Allocate mem for host\n",
        "    float *h_M = (float*)malloc(sizeof(float) * M * N);\n",
        "    float *h_N = (float*)malloc(sizeof(float) * M * K);\n",
        "    float *h_P = (float*)malloc(sizeof(float) * N * K);\n",
        "\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < N * M; i++) {\n",
        "        h_M[i] = rand() / (float)RAND_MAX;\n",
        "    }\n",
        "    for (int i = 0; i < M * K; i++) {\n",
        "        h_N[i] = rand() / (float)RAND_MAX;\n",
        "    }\n",
        "\n",
        "    //Allocate mem for device\n",
        "    float *d_M, *d_N, *d_P;\n",
        "    cudaMalloc((void**)&d_N, sizeof(float) * M * K);\n",
        "    cudaMalloc((void**)&d_M, sizeof(float)* M * N);\n",
        "    cudaMalloc((void**)&d_P, sizeof(float) * N * K);\n",
        "    cudaMemcpy(d_M, h_M, sizeof(float) * M * N, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_N, h_N, sizeof(float) * M * K, cudaMemcpyHostToDevice);\n",
        "        \n",
        "    //Set up config for parallelization\n",
        "    dim3 dimGrid(ceil(K / (float)TITLE_SIZE), ceil(N / (float)TITLE_SIZE), 1);\n",
        "    dim3 dimBlock(TITLE_SIZE, TITLE_SIZE, 1);\n",
        "\n",
        "    float elapsed_time;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    //Invoke Kernel\n",
        "    MatrixMultTiled<<<dimGrid, dimBlock>>>(d_M, d_N, d_P, N, M, K);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&elapsed_time, start, stop);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    cudaMemcpy(h_P, d_P, sizeof(float)*N*K, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Kernel execution time: %f ms\\n\", elapsed_time);\n",
        "\n",
        "    //Host mem free\n",
        "    free(h_M);\n",
        "    free(h_N);\n",
        "    free(h_P);\n",
        "\n",
        "    //Device mem free\n",
        "    cudaFree(d_M);\n",
        "    cudaFree(d_N);\n",
        "    cudaFree(d_P);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
